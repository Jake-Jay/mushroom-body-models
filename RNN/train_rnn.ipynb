{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python371jvsc74a57bd05dd3393c6d60197357608c31af6e4b44ec9cf37f50655e1f1b46e63a63f4f911",
   "display_name": "Python 3.7.1 64-bit ('i2dl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter()\n",
    "\n",
    "from dataset import MushroomBodyDataset \n",
    "from rnn import NeuralRNNModule, NeuralRNN\n",
    "\n",
    "%load_ext autoreload\n",
    "%aimport dataset, rnn\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datum shape for input and output is (timesteps, neurons): torch.Size([3, 15])\n"
     ]
    }
   ],
   "source": [
    "dataset = MushroomBodyDataset()\n",
    "\n",
    "print(f'Datum shape for input and output is (timesteps, neurons): {dataset[0][\"dan\"].shape}')\n",
    "# dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train dataset size: 5400\nVal dataset size: 1800\nTest dataset size: 1800\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(dataset.data)\n",
    "train_percentage, val_percentage = 0.6, 0.2\n",
    "test_percentage = 1 - train_percentage - val_percentage\n",
    "\n",
    "lengths = [\n",
    "    int(train_percentage * num_samples), \n",
    "    int(val_percentage * num_samples),\n",
    "    int(test_percentage * num_samples),\n",
    "]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, lengths, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Val dataset size: {len(val_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input shape torch.Size([1, 3, 15])\nTesting built in RNN\ntorch.Size([1, 3, 15])\ntorch.Size([1, 1, 15])\nTesting my RNN\ntorch.Size([1, 3, 15])\ntorch.Size([1, 1, 15])\n"
     ]
    }
   ],
   "source": [
    "# Single sample test\n",
    "sample = train_dataset[0]['dan']\n",
    "timesteps, input_features = sample.shape\n",
    "\n",
    "sample = sample.unsqueeze(0)\n",
    "print('Input shape', sample.shape)\n",
    "\n",
    "print('Testing built in RNN')\n",
    "rnn = nn.RNN(\n",
    "    input_size=15, \n",
    "    hidden_size=15,\n",
    "    num_layers=1,\n",
    "    nonlinearity='tanh',\n",
    "    bias=True,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "output1, h_n1 = rnn(sample)\n",
    "\n",
    "# (batch, seq_len, 1*hidden_size) = (1,3,15)\n",
    "# tensor containing the output features (h_t) from the last layer of the RNN\n",
    "print(output1.shape)\n",
    "\n",
    "# num_layers * num_directions, batch, hidden_size = (1*1, 1, 15)\n",
    "# Final hidden state : tensor containing the hidden state for t = seq_len.\n",
    "print(h_n1.shape)\n",
    "\n",
    "print('Testing my RNN')\n",
    "my_rnn = NeuralRNN(\n",
    "    input_size=15, \n",
    "    hidden_size=15\n",
    ")\n",
    "\n",
    "output2, h_n2 = rnn(sample)\n",
    "print(output2.shape)\n",
    "print(h_n2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.1817, -0.2094,  0.3375,  0.0886,  0.1764,  0.4102,  0.1239,\n",
       "           0.0894, -0.2626,  0.0224,  0.0348, -0.0811,  0.2149,  0.3996,\n",
       "          -0.0046],\n",
       "         [ 0.3567, -0.3992,  0.2869, -0.1424,  0.1475,  0.5080, -0.1094,\n",
       "           0.2515, -0.4954,  0.0657, -0.1514,  0.0449,  0.1507,  0.2951,\n",
       "           0.1954],\n",
       "         [ 0.3965, -0.3447,  0.2989, -0.0626,  0.1623,  0.4247,  0.0175,\n",
       "           0.3142, -0.3226,  0.2091, -0.1140, -0.2171,  0.1495,  0.1899,\n",
       "           0.0581]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.1817, -0.2094,  0.3375,  0.0886,  0.1764,  0.4102,  0.1239,\n",
       "           0.0894, -0.2626,  0.0224,  0.0348, -0.0811,  0.2149,  0.3996,\n",
       "          -0.0046],\n",
       "         [ 0.3567, -0.3992,  0.2869, -0.1424,  0.1475,  0.5080, -0.1094,\n",
       "           0.2515, -0.4954,  0.0657, -0.1514,  0.0449,  0.1507,  0.2951,\n",
       "           0.1954],\n",
       "         [ 0.3965, -0.3447,  0.2989, -0.0626,  0.1623,  0.4247,  0.0175,\n",
       "           0.3142, -0.3226,  0.2091, -0.1140, -0.2171,  0.1495,  0.1899,\n",
       "           0.0581]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([8, 3, 15])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "output, h_n = my_rnn(batch['dan'])\n",
    "\n",
    "print(output.shape)\n",
    "# output"
   ]
  },
  {
   "source": [
    "# Lets Try Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralRNNModule(\n",
    "    input_dim=15,\n",
    "    hidden_dim=15, \n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 15])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Check that the forward pass does what I expect\n",
    "output = model(batch['dan'])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name     | Type      | Params\n",
      "---------------------------------------\n",
      "0 | rnn      | NeuralRNN | 480   \n",
      "1 | loss_fcn | MSELoss   | 0     \n",
      "---------------------------------------\n",
      "480       Trainable params\n",
      "0         Non-trainable params\n",
      "480       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "Epoch 0:   1%|          | 10/900 [00:00<00:13, 65.54it/s, loss=0.0844, v_num=2]/anaconda3/envs/i2dl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/anaconda3/envs/i2dl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  75%|███████▌  | 675/900 [00:12<00:04, 54.56it/s, loss=0.00457, v_num=2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  76%|███████▌  | 680/900 [00:12<00:04, 54.11it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  78%|███████▊  | 700/900 [00:12<00:03, 55.27it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  80%|████████  | 720/900 [00:12<00:03, 56.33it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  83%|████████▎ | 743/900 [00:12<00:02, 57.67it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  85%|████████▌ | 766/900 [00:12<00:02, 58.96it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  88%|████████▊ | 789/900 [00:13<00:01, 60.17it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  90%|█████████ | 812/900 [00:13<00:01, 61.31it/s, loss=0.00457, v_num=2]\n",
      "Validating:  62%|██████▏   | 140/225 [00:00<00:00, 186.90it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 835/900 [00:13<00:01, 62.21it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  95%|█████████▌| 858/900 [00:13<00:00, 62.97it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0:  98%|█████████▊| 881/900 [00:13<00:00, 64.11it/s, loss=0.00457, v_num=2]\n",
      "Epoch 0: 100%|██████████| 900/900 [00:13<00:00, 64.95it/s, loss=0.00457, v_num=2]\n",
      "Epoch 1:  75%|███████▌  | 675/900 [00:10<00:03, 65.47it/s, loss=0.00413, v_num=2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 690/900 [00:10<00:03, 65.37it/s, loss=0.00413, v_num=2]\n",
      "Validating:   8%|▊         | 17/225 [00:00<00:01, 160.18it/s]\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 713/900 [00:10<00:02, 66.72it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1:  82%|████████▏ | 736/900 [00:10<00:02, 67.97it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1:  84%|████████▍ | 759/900 [00:10<00:02, 69.32it/s, loss=0.00413, v_num=2]\n",
      "Validating:  39%|███▉      | 88/225 [00:00<00:00, 167.17it/s]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 782/900 [00:11<00:01, 70.34it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1:  89%|████████▉ | 805/900 [00:11<00:01, 71.67it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1:  92%|█████████▏| 828/900 [00:11<00:00, 73.00it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1:  95%|█████████▍| 851/900 [00:11<00:00, 74.16it/s, loss=0.00413, v_num=2]\n",
      "Validating:  80%|████████  | 180/225 [00:01<00:00, 168.11it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 874/900 [00:11<00:00, 75.13it/s, loss=0.00413, v_num=2]\n",
      "Epoch 1: 100%|██████████| 900/900 [00:11<00:00, 75.90it/s, loss=0.00413, v_num=2]\n",
      "Epoch 2:  67%|██████▋   | 600/900 [00:11<00:05, 50.87it/s, loss=0.00314, v_num=2]/anaconda3/envs/i2dl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}